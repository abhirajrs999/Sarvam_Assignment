{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preparation**"
      ],
      "metadata": {
        "id": "pvD00Alx6r_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used pre-trained FastText Wikipedia embeddings for English and Hindi (300-dimensional vectors) to avoid training from scratch​. The vocabulary in each language is truncated to the top 100,000 most frequent words for efficiency, as suggested in the assignment"
      ],
      "metadata": {
        "id": "gPGYrJJB7A_L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vszi_anAhKyU"
      },
      "outputs": [],
      "source": [
        "!wget -q -O cc.en.300.vec.gz https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "!wget -q -O cc.hi.300.vec.gz https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz\n",
        "!gunzip cc.en.300.vec.gz\n",
        "!gunzip cc.hi.300.vec.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_embeddings(file_path, top_n=100000):\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        header = f.readline()\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= top_n: break\n",
        "            parts = line.rstrip().split(' ')\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            word = parts[0]\n",
        "            vec = np.array(parts[1:], dtype=float)\n",
        "            embeddings[word] = vec\n",
        "    return embeddings\n",
        "\n",
        "# Loading top 100k English and Hindi embeddings\n",
        "eng_embeddings = load_embeddings(\"cc.en.300.vec\", top_n=100000)\n",
        "hi_embeddings  = load_embeddings(\"cc.hi.300.vec\", top_n=100000)\n",
        "print(f\"Loaded {len(eng_embeddings)} English and {len(hi_embeddings)} Hindi word vectors.\")\n"
      ],
      "metadata": {
        "id": "cbb_FDBZhOb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6574f15c-ed99-4f44-8d4c-d7c2b0700de9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100000 English and 100000 Hindi word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_embeddings(embed_dict):\n",
        "    for word, vec in embed_dict.items():\n",
        "        embed_dict[word] = vec / np.linalg.norm(vec)\n",
        "normalize_embeddings(eng_embeddings)\n",
        "normalize_embeddings(hi_embeddings)\n"
      ],
      "metadata": {
        "id": "ZXX_5ivUhUsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting a list of word translation pairs from the MUSE dataset to use as a bilingual lexicon for supervised alignment."
      ],
      "metadata": {
        "id": "juzt9QR481qS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download en-hi.txt\n",
        "!wget -q -O en-hi.txt https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt"
      ],
      "metadata": {
        "id": "K40YOntShWIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "all_pairs = []\n",
        "with open(\"en-hi.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        parts = line.split()\n",
        "        if len(parts) != 2:\n",
        "            continue\n",
        "        en_word, hi_word = parts\n",
        "        all_pairs.append((en_word, hi_word))\n",
        "\n",
        "print(f\"Total pairs read: {len(all_pairs)}\")\n",
        "\n",
        "# Shuddling for random split\n",
        "random.shuffle(all_pairs)\n"
      ],
      "metadata": {
        "id": "iMPewb6BlY70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "387dd547-e792-433a-e7b6-aaadff47d217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pairs read: 38221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embedding Alignment**"
      ],
      "metadata": {
        "id": "hONiaRkl9vJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First 5 pairs:\", all_pairs[:5])"
      ],
      "metadata": {
        "id": "3Zvplevslab4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f3c8672-a421-417b-903b-9c4ebc0b0ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 pairs: [('modernity', 'आधुनिकता'), ('vibrant', 'हर्षोल्लास'), ('aligarh', 'अलीगढ़'), ('curtail', 'काटना'), ('brahman', 'ब्रह्म')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding test_pairs to find in which way we can split\n",
        "filtered_pairs = []\n",
        "for (en_word, hi_word) in all_pairs:\n",
        "    if en_word in eng_embeddings and hi_word in hi_embeddings:\n",
        "        filtered_pairs.append((en_word, hi_word))\n",
        "\n"
      ],
      "metadata": {
        "id": "0u8foR-goVFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(filtered_pairs))"
      ],
      "metadata": {
        "id": "dWb7gNVQoc92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55bcd8f-cf68-40f5-9665-83014ac69dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18972\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting into train and test**"
      ],
      "metadata": {
        "id": "2_2Wh2mTlfDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For train we use 17000 and for test 1500\n",
        "n_train = 17000\n",
        "n_test  = 1500\n",
        "\n",
        "# To Ensure we have enough pairs\n",
        "assert len(filtered_pairs) >= (n_train + n_test), \"Not enough total pairs to split!\"\n",
        "\n",
        "train_pairs = filtered_pairs[:n_train]\n",
        "test_pairs  = filtered_pairs[n_train : n_train + n_test]\n",
        "\n",
        "print(f\"Train pairs: {len(train_pairs)}\")\n",
        "print(f\"Test pairs:  {len(test_pairs)}\")\n"
      ],
      "metadata": {
        "id": "OJnj4koZhXcB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "413292b5-622b-4dc9-cfce-c995a4f7310f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train pairs: 17000\n",
            "Test pairs:  1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constructing matrix X: English & Y: Hindi for training pairs\n",
        "dim = 300  # embedding dimensionality\n",
        "n_train = len(train_pairs)\n",
        "X = np.zeros((n_train, dim))\n",
        "Y = np.zeros((n_train, dim))\n",
        "for i, (en_word, hi_word) in enumerate(train_pairs):\n",
        "    X[i] = eng_embeddings[en_word]\n",
        "    Y[i] = hi_embeddings[hi_word]\n",
        "\n",
        "print(\"X shape:\", X.shape, \"Y shape:\", Y.shape)\n"
      ],
      "metadata": {
        "id": "JCS6vuLQhYG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e727d28a-c883-4296-9a4f-571962e42f7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (17000, 300) Y shape: (17000, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing SVD of X^T Y\n",
        "XY = X.T.dot(Y)  # shape (300, 300)\n",
        "U, S, Vt = np.linalg.svd(XY)\n",
        "W = U.dot(Vt)  # orthogonal mapping matrix\n",
        "# Ensure W is proper orthogonal (determinant = 1) by sign correction if needed\n",
        "if np.linalg.det(W) < 0:\n",
        "    U[:,-1] *= -1\n",
        "    W = U.dot(Vt)\n",
        "\n",
        "# Verify orthonormality: W^T W = I?\n",
        "identity_check = W.T.dot(W)\n",
        "print(\"Orthonormal check (W^T W): off-diagonal mean =\",\n",
        "      np.mean(np.abs(identity_check - np.eye(dim))))\n"
      ],
      "metadata": {
        "id": "8hbu7bpWhZ-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea3ab86-89c3-415c-d6ad-138abf5f44ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orthonormal check (W^T W): off-diagonal mean = 1.6367669125766966e-16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply W to all English vectors (for top vocab)\n",
        "eng_words = list(eng_embeddings.keys())\n",
        "eng_matrix = np.array([eng_embeddings[w] for w in eng_words])\n",
        "# mapped English vectors\n",
        "eng_matrix_aligned = eng_matrix.dot(W)\n",
        "# Hindi matrix for target words:\n",
        "hi_words = list(hi_embeddings.keys())\n",
        "hi_matrix = np.array([hi_embeddings[w] for w in hi_words])\n"
      ],
      "metadata": {
        "id": "lcjfRvd8hefo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation**"
      ],
      "metadata": {
        "id": "SSBsqz46ABAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import norm\n",
        "\n",
        "# Normalize aligned English and Hindi matrices\n",
        "X_aligned = eng_matrix_aligned / norm(eng_matrix_aligned, axis=1, keepdims=True)\n",
        "Y_hindi   = hi_matrix / norm(hi_matrix, axis=1, keepdims=True)\n",
        "\n",
        "# Create index mappings for Hindi words\n",
        "hi_index = {w:i for i,w in enumerate(hi_words)}\n",
        "\n",
        "def precision_at_k(k=1):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for en_word, hi_word in test_pairs:\n",
        "        if en_word not in eng_embeddings or hi_word not in hi_embeddings:\n",
        "            continue\n",
        "        total += 1\n",
        "        # Get English vector and compute cosine similarities to all Hindi vectors\n",
        "        x = eng_embeddings[en_word].dot(W)\n",
        "        # Compute cosine similarity with all Hindi vectors (dot product is done since normalized)\n",
        "        sims = x.dot(Y_hindi.T)\n",
        "        # Finding top-k Hindi indices\n",
        "        neighbors = np.argpartition(-sims, k)[:k]\n",
        "        # Checking if the correct Hindi word is in the top-k\n",
        "        hi_idx = hi_index.get(hi_word, None)\n",
        "        if hi_idx is not None and hi_idx in neighbors:\n",
        "            correct += 1\n",
        "    return correct, total\n",
        "\n",
        "# Evaluate precision@1 and precision@5\n",
        "correct1, total = precision_at_k(k=1)\n",
        "correct5, _     = precision_at_k(k=5)\n",
        "print(f\"Precision@1 = {correct1/total*100:.2f}%, Precision@5 = {correct5/total*100:.2f}% (on {total} test words)\")\n"
      ],
      "metadata": {
        "id": "EDqLloB2hhIw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69a697cd-1e9e-420b-efca-df7acaabe3f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision@1 = 26.07%, Precision@5 = 51.87% (on 1500 test words)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example translations\n",
        "sample_words = [\"dog\", \"city\", \"beautiful\", \"language\", \"India\"]\n",
        "for en in sample_words:\n",
        "    if en in eng_embeddings:\n",
        "        x = eng_embeddings[en].dot(W)\n",
        "        sims = x.dot(Y_hindi.T)\n",
        "        top5_idx = np.argsort(-sims)[:5]\n",
        "        top5_hindi = [hi_words[i] for i in top5_idx]\n",
        "        print(f\"{en} -> {top5_hindi}\")\n"
      ],
      "metadata": {
        "id": "8PV9heMPhkC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ade6bb9-fbcd-4737-ab90-7b437ce91638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dog -> ['कुत्ते', 'कुत्ता', 'कुत्तों', 'कुत्तो', 'कुता']\n",
            "city -> ['शहर', 'नगर', 'महानगर', 'राजधानी', 'इलाके']\n",
            "beautiful -> ['सुंदर', 'खूबसूरत', 'सुन्दर', 'खुबसूरत', 'ख़ूबसूरत']\n",
            "language -> ['भाषा', 'भाषाओं', 'अंग्रेजी', 'भाषाएं', 'मातृभाषा']\n",
            "India -> ['भारत', 'महाराष्ट्र', 'भारतीय', 'एशिया', 'देश']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Just checking cosine similarity of random words\n",
        "pairs_to_check = [(\"dog\",\"कुत्ता\"), (\"red\",\"लाल\"), (\"one\",\"एक\"), (\"city\",\"शहर\"), (\"water\",\"पानी\")]\n",
        "for en, hi in pairs_to_check:\n",
        "    cos_sim = eng_embeddings[en].dot(W).dot(hi_embeddings[hi])\n",
        "    rand_hi = np.random.choice(hi_words)\n",
        "    cos_sim_rand = eng_embeddings[en].dot(W).dot(hi_embeddings[rand_hi])\n",
        "    print(f\"cosine({en}, {hi}) = {cos_sim:.3f}, cosine({en}, '{rand_hi}') = {cos_sim_rand:.3f}\")\n"
      ],
      "metadata": {
        "id": "CVy9l5zkhl98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68a33a37-5bb6-4d3c-b5f5-daa34153d231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cosine(dog, कुत्ता) = 0.656, cosine(dog, 'उड़ने') = 0.211\n",
            "cosine(red, लाल) = 0.627, cosine(red, '362') = 0.056\n",
            "cosine(one, एक) = 0.487, cosine(one, 'कर्व') = 0.078\n",
            "cosine(city, शहर) = 0.768, cosine(city, '1.80') = 0.069\n",
            "cosine(water, पानी) = 0.826, cosine(water, 'नॉर्स') = 0.070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ablation Study done below**"
      ],
      "metadata": {
        "id": "TxHiJXXXAMNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def build_training_matrices(dict_pairs, eng_emb, hi_emb):\n",
        "    \"\"\"\n",
        "     Given a list of (en_word, hi_word) pairs, build the X (English) and Y (Hindi) matrices.\n",
        "     Returns (X, Y) with shape (n, dim).\n",
        "    \"\"\"\n",
        "    X, Y = [], []\n",
        "    for en_word, hi_word in dict_pairs:\n",
        "        if en_word in eng_emb and hi_word in hi_emb:\n",
        "            X.append(eng_emb[en_word])\n",
        "            Y.append(hi_emb[hi_word])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "def compute_procrustes_mapping(X, Y):\n",
        "    \"\"\"\n",
        "     Orthogonal Procrustes. Given X, Y in R^(n x d),\n",
        "     compute W in R^(d x d).\n",
        "    \"\"\"\n",
        "    M = X.T @ Y\n",
        "    U, S, Vt = np.linalg.svd(M)\n",
        "    W = U @ Vt\n",
        "    # fix reflection\n",
        "    if np.linalg.det(W) < 0:\n",
        "        U[:, -1] *= -1\n",
        "        W = U @ Vt\n",
        "    return W\n",
        "\n",
        "def evaluate_word_translation(W, test_pairs, eng_emb, hi_emb, k=1):\n",
        "    \"\"\"\n",
        "     Evaluate precision at k for the linear map W from English->Hindi.\n",
        "     test_pairs: list of (en_word, hi_word).\n",
        "     W: (d x d) matrix.\n",
        "     Returns (correct, total) for that k.\n",
        "    \"\"\"\n",
        "    hi_words = list(hi_emb.keys())\n",
        "    hi_matrix = np.array([hi_emb[w] for w in hi_words])\n",
        "    hi_index  = {w: i for i,w in enumerate(hi_words)}\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for en_word, hi_word in test_pairs:\n",
        "        if en_word not in eng_emb or hi_word not in hi_emb:\n",
        "            continue\n",
        "        total += 1\n",
        "        mapped_vec = eng_emb[en_word].dot(W)\n",
        "        mapped_vec /= (np.linalg.norm(mapped_vec) + 1e-9)\n",
        "        sims = hi_matrix @ mapped_vec\n",
        "        nn_indices = np.argpartition(-sims, k)[:k]\n",
        "        if hi_index[hi_word] in nn_indices:\n",
        "            correct += 1\n",
        "    return correct, total\n",
        "\n",
        "def ablation_experiment(sizes, train_pairs, test_pairs, eng_emb, hi_emb):\n",
        "    results = {}\n",
        "    for size in sizes:\n",
        "        # 1) subset the training pairs\n",
        "        sub_pairs = train_pairs[:size]\n",
        "\n",
        "        # 2) build X, Y\n",
        "        X, Y = build_training_matrices(sub_pairs, eng_emb, hi_emb)\n",
        "\n",
        "        # 3) procrustes\n",
        "        W = compute_procrustes_mapping(X, Y)\n",
        "\n",
        "        # 4) evaluate\n",
        "        c1, tot = evaluate_word_translation(W, test_pairs, eng_emb, hi_emb, k=1)\n",
        "        c5, _   = evaluate_word_translation(W, test_pairs, eng_emb, hi_emb, k=5)\n",
        "        p1 = c1 / tot * 100\n",
        "        p5 = c5 / tot * 100\n",
        "\n",
        "        results[size] = (p1, p5)\n",
        "        print(f\"Dictionary Size={size}: P@1={p1:.2f}%, P@5={p5:.2f}%  (N={tot})\")\n",
        "    return results\n",
        "\n",
        "sizes_to_test = [5000, 10000, 15000]   # as required\n",
        "ablation_results = ablation_experiment(\n",
        "    sizes_to_test,\n",
        "    train_pairs,\n",
        "    test_pairs,\n",
        "    eng_embeddings,\n",
        "    hi_embeddings\n",
        ")\n"
      ],
      "metadata": {
        "id": "Snjf0bDEhqua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18945d3c-c38b-4a47-fef6-f6f9ce88c25a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary Size=5000: P@1=24.40%, P@5=49.00%  (N=1500)\n",
            "Dictionary Size=10000: P@1=26.07%, P@5=50.53%  (N=1500)\n",
            "Dictionary Size=15000: P@1=26.40%, P@5=51.67%  (N=1500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_ablation_results(results):\n",
        "    # results is a dict with keys as dictionary sizes and values as (p1, p5)\n",
        "    sizes = sorted(results.keys())\n",
        "    p1_vals = [results[size][0] for size in sizes]\n",
        "    p5_vals = [results[size][1] for size in sizes]\n",
        "\n",
        "    x = range(len(sizes))\n",
        "    width = 0.35\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.bar([i - width/2 for i in x], p1_vals, width=width, label=\"Precision@1\")\n",
        "    plt.bar([i + width/2 for i in x], p5_vals, width=width, label=\"Precision@5\")\n",
        "\n",
        "    plt.xticks(x, [str(size) for size in sizes])\n",
        "    plt.xlabel(\"Training Dictionary Size (word pairs)\")\n",
        "    plt.ylabel(\"Precision (%)\")\n",
        "    plt.title(\"Ablation Study: Impact of Bilingual Lexicon Size\")\n",
        "    plt.legend()\n",
        "    plt.ylim([0, 100])\n",
        "    plt.show()\n",
        "\n",
        "plot_ablation_results(ablation_results)\n"
      ],
      "metadata": {
        "id": "s93cCNAH5WFl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "5977e8b3-3a6e-45fe-886f-9564a077cb32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optional Extra Credit**"
      ],
      "metadata": {
        "id": "JFipVO0HhvvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mapper (W):** a linear transformation matrix (size 300×300) that maps source (English) embeddings into the target (Hindi) space. We constrain W to be orthogonal (a rotation), so it preserves distances and vector norms​. Initially, W can be randomized (e.g., near identity or a random orthonormal matrix).  \n",
        "**Discriminator (D):** a binary classifier that takes a vector and predicts whether it comes from the mapped source distribution (W * English word) or from the actual target (Hindi) distribution​. We use a simple feed-forward network (MLP) with a couple of hidden layers and LeakyReLU activations, similar to Conneau et al. For our discriminator, we use a multilayer perceptron with two hidden layers of size **2048**, and **Leaky-ReLU activation functions**. We use stochastic gradient descent with a batch size of 32, a learning rate of 0.1.The discriminator is trained to distinguish languages, while W is trained to fool it, making the mapped English vectors indistinguishable from Hindi vectors (a two-player game, like a GAN)​"
      ],
      "metadata": {
        "id": "HuDftrVGBsWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Max_vocab set for this assignment\n",
        "max_vocab = 100000\n",
        "\n",
        "en_vec_path = '/content/cc.en.300.vec'\n",
        "hi_vec_path = '/content/cc.hi.300.vec'\n",
        "\n",
        "def load_embeddings(file_path, max_vocab=None):\n",
        "    embeddings = []\n",
        "    word2idx = {}\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        first_line = f.readline()\n",
        "        try:\n",
        "            total_words, dim = map(int, first_line.split())\n",
        "        except:\n",
        "            f.seek(0)\n",
        "            # Just read lines and let max_vocab handle it\n",
        "            total_words, dim = None, None\n",
        "\n",
        "        loaded_count = 0\n",
        "        for i, line in enumerate(f):\n",
        "            # If we have a max_vocab, stop early\n",
        "            if max_vocab is not None and i >= max_vocab:\n",
        "                break\n",
        "\n",
        "            parts = line.rstrip().split(' ')\n",
        "            if len(parts) < 301:  # must have at least 1 token + 300 floats\n",
        "                continue\n",
        "            word = parts[0]\n",
        "            vec = np.array(parts[1:], dtype=float)\n",
        "            # Just in case there iss mismatch in dim from the file\n",
        "            if dim is None:\n",
        "                dim = len(vec)\n",
        "            elif len(vec) != dim:\n",
        "                continue\n",
        "\n",
        "            word2idx[word] = len(embeddings)\n",
        "            embeddings.append(vec)\n",
        "            loaded_count += 1\n",
        "\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    print(f\"Loaded {loaded_count} words from {file_path} with dim={dim}\")\n",
        "    return embeddings, word2idx\n",
        "\n",
        "def normalize_embeddings(embeddings):\n",
        "    \"\"\"Doing row-wise L2 normalization for simplicity.\"\"\"\n",
        "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    embeddings = embeddings / (norms + 1e-8)\n",
        "    return embeddings\n",
        "\n",
        "print(\"Loading English embeddings...\")\n",
        "en_embeddings, en_word2idx = load_embeddings(en_vec_path, max_vocab=max_vocab)\n",
        "print(\"Loading Hindi embeddings...\")\n",
        "hi_embeddings, hi_word2idx = load_embeddings(hi_vec_path, max_vocab=max_vocab)\n",
        "\n",
        "# Normalizing the embeddings\n",
        "en_embeddings = normalize_embeddings(en_embeddings)\n",
        "hi_embeddings = normalize_embeddings(hi_embeddings)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "src_emb = torch.from_numpy(en_embeddings.astype(np.float32)).to(device)\n",
        "tgt_emb = torch.from_numpy(hi_embeddings.astype(np.float32)).to(device)\n",
        "\n",
        "mapper = nn.Linear(300, 300, bias=False).to(device)\n",
        "with torch.no_grad():\n",
        "    mapper.weight.copy_(torch.eye(300))\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=2048):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, 1)\n",
        "        self.act = nn.LeakyReLU(0.2)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(self.act(self.layer1(x)))\n",
        "        x = self.dropout(self.act(self.layer2(x)))\n",
        "        return self.out(x)\n",
        "\n",
        "discriminator = Discriminator(300).to(device)\n",
        "\n",
        "optimizer_D = optim.SGD(discriminator.parameters(), lr=0.1)\n",
        "optimizer_W = optim.SGD(mapper.parameters(), lr=0.1)\n",
        "\n",
        "batch_size = 32\n",
        "n_epochs = 5\n",
        "print_interval = 1000\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    n_batches = 2000\n",
        "    for b in range(1, n_batches+1):\n",
        "        # sample random\n",
        "        src_idx = torch.randint(0, src_emb.size(0), (batch_size,), device=device)\n",
        "        tgt_idx = torch.randint(0, tgt_emb.size(0), (batch_size,), device=device)\n",
        "        src_batch = src_emb[src_idx]\n",
        "        tgt_batch = tgt_emb[tgt_idx]\n",
        "\n",
        "        # map src\n",
        "        mapped_src = mapper(src_batch)\n",
        "\n",
        "        # ----- Training Discriminator -----\n",
        "        discriminator.train()\n",
        "        optimizer_D.zero_grad()\n",
        "        D_input = torch.cat([mapped_src.detach(), tgt_batch], dim=0)\n",
        "        D_labels = torch.cat([\n",
        "            torch.zeros(batch_size, 1, device=device),\n",
        "            torch.ones(batch_size, 1, device=device)\n",
        "        ], dim=0)\n",
        "        D_logits = discriminator(D_input)\n",
        "        D_loss = F.binary_cross_entropy_with_logits(D_logits, D_labels)\n",
        "        D_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # ----- Train Mapper (Generator) -----\n",
        "        optimizer_W.zero_grad()\n",
        "        # Want mapped_src to be classified as real (1)\n",
        "        # and actual tgt_batch as fake (0)\n",
        "        fool_input = torch.cat([mapper(src_batch), tgt_batch], dim=0)\n",
        "        fool_labels = torch.cat([\n",
        "            torch.ones(batch_size, 1, device=device),\n",
        "            torch.zeros(batch_size, 1, device=device)\n",
        "        ], dim=0)\n",
        "        fool_logits = discriminator(fool_input)\n",
        "        W_loss = F.binary_cross_entropy_with_logits(fool_logits, fool_labels)\n",
        "        W_loss.backward()\n",
        "        optimizer_W.step()\n",
        "\n",
        "        # Orthonormalize W\n",
        "        with torch.no_grad():\n",
        "            W = mapper.weight.data\n",
        "            beta = 0.01\n",
        "            WWt = W @ W.t()\n",
        "            mapper.weight.copy_((1+beta)*W - beta*(WWt @ W))\n",
        "\n",
        "        if b % print_interval == 0:\n",
        "            D_acc = 0\n",
        "            # Quick accuracy estimate\n",
        "            with torch.no_grad():\n",
        "                '''\n",
        "                 First half are mapped_src => label=0\n",
        "                 Second half are tgt => label=1\n",
        "                 if logistic > 0 => predicted=1\n",
        "                '''\n",
        "                preds = (D_logits.sigmoid() >= 0.5).int()\n",
        "                correct_first_half = (preds[:batch_size] == 0).sum()\n",
        "                correct_second_half = (preds[batch_size:] == 1).sum()\n",
        "                D_acc = (correct_first_half + correct_second_half).item() / (2*batch_size)\n",
        "            print(f\"Epoch {epoch} Batch {b}: D_loss={D_loss.item():.4f}, W_loss={W_loss.item():.4f}, D_acc={D_acc:.2f}\")\n",
        "\n",
        "# After training, retrieve the final W\n",
        "W_torch = mapper.weight.detach().cpu().numpy()  # shape (300,300)\n",
        "# Note: PyTorch's Linear applies x * W^T if x is row vector\n",
        "W_np = W_torch.T  # shape (300,300) for standard \"row_vec.dot(W)\" usage\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3sP_DytmtDY",
        "outputId": "f82e2c13-50a1-4ff8-fc39-58aa47e671e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading English embeddings...\n",
            "Loaded 100000 words from /content/cc.en.300.vec with dim=300\n",
            "Loading Hindi embeddings...\n",
            "Loaded 100000 words from /content/cc.hi.300.vec with dim=300\n",
            "Using device: cuda\n",
            "Epoch 1 Batch 1000: D_loss=0.6929, W_loss=0.6932, D_acc=0.55\n",
            "Epoch 1 Batch 2000: D_loss=0.6690, W_loss=0.7210, D_acc=0.81\n",
            "Epoch 2 Batch 1000: D_loss=0.5921, W_loss=0.9145, D_acc=0.75\n",
            "Epoch 2 Batch 2000: D_loss=0.6438, W_loss=1.1078, D_acc=0.55\n",
            "Epoch 3 Batch 1000: D_loss=0.5500, W_loss=0.9732, D_acc=0.80\n",
            "Epoch 3 Batch 2000: D_loss=0.5324, W_loss=1.0671, D_acc=0.81\n",
            "Epoch 4 Batch 1000: D_loss=0.5552, W_loss=1.0274, D_acc=0.75\n",
            "Epoch 4 Batch 2000: D_loss=0.5911, W_loss=1.1350, D_acc=0.55\n",
            "Epoch 5 Batch 1000: D_loss=0.6035, W_loss=1.1000, D_acc=0.66\n",
            "Epoch 5 Batch 2000: D_loss=0.5131, W_loss=1.1681, D_acc=0.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In each iteration, we sample a batch of English and Hindi embeddings.\n",
        "We first update the discriminator D to better distinguish true Hindi from mapped English embeddings.\n",
        "\n",
        "Then, we update the mapper W to minimize the inverse of that loss — i.e., to confuse the discriminator.\n",
        "\n",
        "We also enforce an orthogonal constraint on W during training for stability and to preserve the monolingual structure.\n",
        "This is done through a projection step applied after every mapper update:\n",
        "\n",
        "𝑊\n",
        "←\n",
        "(\n",
        "1\n",
        "+\n",
        "𝛽\n",
        ")\n",
        "𝑊\n",
        "−\n",
        "𝛽\n",
        "(\n",
        "𝑊\n",
        "𝑊\n",
        "𝑇\n",
        ")\n",
        "\n",
        "This update pushes\n",
        "𝑊\n",
        "W toward the nearest orthogonal matrix, with\n",
        "\n",
        "β=0.01 typically.\n",
        "\n",
        "We use PyTorch for training. The discriminator's loss is binary cross-entropy (or negative log-likelihood), classifying source vs. target embeddings.\n",
        "The mapper is trained with inverse labels, so mapped source embeddings are encouraged to look like target embeddings (and vice versa) — essentially fooling the discriminator."
      ],
      "metadata": {
        "id": "CI08THoqE24X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Check how many test pairs are in-vocab now\n",
        "num_valid = 0\n",
        "for (en_w, hi_w) in test_pairs:\n",
        "    if en_w in en_word2idx and hi_w in hi_word2idx:\n",
        "        num_valid += 1\n",
        "print(\"Number of test pairs actually in both vocabularies:\", num_valid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMv1t9qYndlt",
        "outputId": "c1d3d601-ee31-4bf4-dd3b-53e433d5d7af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test pairs actually in both vocabularies: 1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Major Steps\n",
        "\n",
        "1. **Mapping**  \n",
        "   We apply the learned matrix W_np to the English embeddings (en_embeddings.dot(W_np)).  \n",
        "   Then we renormalize each row to be a unit vector.\n",
        "\n",
        "2. **CSLS Dictionary**  \n",
        "   - We take the top N words from English (mapped_en_embeddings[:N]) and Hindi (hi_embeddings_normed[:N]) by frequency order in the .vec file.  \n",
        "   - We compute **CSLS** mutual nearest neighbors to create a synthetic bilingual dictionary.\n",
        "\n",
        "3. **Orthogonal Procrustes**  \n",
        "   - Once we have a list of matched pairs (i, j), we solve the Procrustes problem to find the best orthogonal matrix `W_refined`.  \n",
        "   - This step typically yields a big boost in accuracy compared to raw adversarial alignment.\n",
        "\n",
        "4. **Final Evaluation**  \n",
        "   - We apply W_refined to **all** English embeddings, producing mapped_en_refined.  \n",
        "   - We then compute a Precision@k metric with a **CSLS retrieval** approach. For each test pair (en_word, hi_word), we check if hi_wor` is in the top-k neighbors under CSLS.\n"
      ],
      "metadata": {
        "id": "nsjA0mm_F30W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Mapping english embeddings with initial W_np\n",
        "\n",
        "mapped_en_embeddings = en_embeddings.dot(W_np)\n",
        "\n",
        "# Normalize row-wise again (good practice to ensure\n",
        "# they are unit vectors for cosine similarity)\n",
        "mapped_en_embeddings /= (\n",
        "    np.linalg.norm(mapped_en_embeddings, axis=1, keepdims=True) + 1e-9\n",
        ")\n",
        "hi_embeddings_normed = hi_embeddings / (\n",
        "    np.linalg.norm(hi_embeddings, axis=1, keepdims=True) + 1e-9\n",
        ")\n",
        "\n",
        "\n",
        "# CSLS utility functions\n",
        "\n",
        "def compute_rT_rS(src_mat, tgt_mat, K=10, batch_size=2048):\n",
        "    \"\"\"\n",
        "    For each row in src_mat, compute average top-K similarity to rows in tgt_mat => rT\n",
        "    For each row in tgt_mat, compute average top-K similarity to rows in src_mat => rS\n",
        "    Both src_mat and tgt_mat are assumed row-wise normalized. (cosine = dot product)\n",
        "    \"\"\"\n",
        "    src_size = src_mat.shape[0]\n",
        "    tgt_size = tgt_mat.shape[0]\n",
        "\n",
        "    rT = np.zeros(src_size, dtype=np.float32)\n",
        "    rS = np.zeros(tgt_size, dtype=np.float32)\n",
        "\n",
        "    # compute rT\n",
        "    start = 0\n",
        "    while start < src_size:\n",
        "        end = min(start + batch_size, src_size)\n",
        "        sims = src_mat[start:end].dot(tgt_mat.T)\n",
        "        if K < tgt_size:\n",
        "            # partial sort to find top-K\n",
        "            top_k = np.partition(sims, kth=-K, axis=1)[:, -K:]\n",
        "\n",
        "            rT_batch = top_k.mean(axis=1)\n",
        "        else:\n",
        "            # if K >= tgt_size, just mean of all\n",
        "            rT_batch = sims.mean(axis=1)\n",
        "        rT[start:end] = rT_batch\n",
        "        start = end\n",
        "\n",
        "    # compute rS\n",
        "    start = 0\n",
        "    while start < tgt_size:\n",
        "        end = min(start + batch_size, tgt_size)\n",
        "        sims = tgt_mat[start:end].dot(src_mat.T)  # shape: (batch, src_size)\n",
        "        if K < src_size:\n",
        "            top_k = np.partition(sims, kth=-K, axis=1)[:, -K:]\n",
        "            rS_batch = top_k.mean(axis=1)\n",
        "        else:\n",
        "            rS_batch = sims.mean(axis=1)\n",
        "        rS[start:end] = rS_batch\n",
        "        start = end\n",
        "\n",
        "    return rT, rS\n",
        "\n",
        "def build_dictionary_csls(src_mat, tgt_mat, K=50, batch_size=2048):\n",
        "    \"\"\"\n",
        "    Build a synthetic dictionary by mutual nearest neighbors under CSLS.\n",
        "    1) Compute rT, rS\n",
        "    2) For each source row i, find best target j => CSLS = 2*cos - rT[i] - rS[j]\n",
        "    3) For each target row j, find best source i\n",
        "    4) Keep only mutual matches => MNN\n",
        "    Returns list of (i, j) index pairs.\n",
        "    \"\"\"\n",
        "    # step 1: compute rT, rS with a smaller K=10 for average knn, or use 50, your choice\n",
        "    rT, rS = compute_rT_rS(src_mat, tgt_mat, K=10)\n",
        "\n",
        "    src_size = src_mat.shape[0]\n",
        "    tgt_size = tgt_mat.shape[0]\n",
        "\n",
        "    # For each source, find best target\n",
        "    src2tgt = np.full(src_size, -1, dtype=int)\n",
        "    # We are also storing the best score\n",
        "    src2tgt_scores = np.full(src_size, -1e9, dtype=float)\n",
        "\n",
        "    start = 0\n",
        "    while start < src_size:\n",
        "        end = min(start + batch_size, src_size)\n",
        "        sims = src_mat[start:end].dot(tgt_mat.T)\n",
        "        # CSLS\n",
        "        # for each row i in [start:end], for each j in [0..tgt_size),\n",
        "        # score = 2*cos - rT[i] - rS[j]\n",
        "        for i in range(end - start):\n",
        "            row_i = start + i\n",
        "            # compute row of CSLS scores\n",
        "            csls_row = 2*sims[i] - rT[row_i] - rS\n",
        "            best_j = np.argmax(csls_row)\n",
        "            best_score = csls_row[best_j]\n",
        "            src2tgt[row_i] = best_j\n",
        "            src2tgt_scores[row_i] = best_score\n",
        "        start = end\n",
        "\n",
        "    # For each target, find best source\n",
        "    tgt2src = np.full(tgt_size, -1, dtype=int)\n",
        "    tgt2src_scores = np.full(tgt_size, -1e9, dtype=float)\n",
        "\n",
        "    start = 0\n",
        "    while start < tgt_size:\n",
        "        end = min(start + batch_size, tgt_size)\n",
        "        sims = tgt_mat[start:end].dot(src_mat.T)\n",
        "        for j in range(end - start):\n",
        "            row_j = start + j\n",
        "            csls_row = 2*sims[j] - rS[row_j] - rT\n",
        "            best_i = np.argmax(csls_row)\n",
        "            best_score = csls_row[best_i]\n",
        "            tgt2src[row_j] = best_i\n",
        "            tgt2src_scores[row_j] = best_score\n",
        "        start = end\n",
        "\n",
        "    # Mutual nearest neighbors\n",
        "    dictionary = []\n",
        "    for i in range(src_size):\n",
        "        j = src2tgt[i]\n",
        "        if j >= 0 and tgt2src[j] == i:\n",
        "            dictionary.append((i, j))\n",
        "    return dictionary\n",
        "\n",
        "def orthogonal_procrustes_refine(dictionary, src_mat, tgt_mat):\n",
        "    \"\"\"\n",
        "    Solve Orthogonal Procrustes using the MNN dictionary:\n",
        "       Argmin_W || W * src - tgt || subject to W^T W = I\n",
        "    where (i, j) in dictionary => row i in src_mat, row j in tgt_mat\n",
        "    Returns refined W (shape 300x300).\n",
        "    \"\"\"\n",
        "    if not dictionary:\n",
        "        print(\"No dictionary pairs found! Returning identity.\")\n",
        "        return np.eye(src_mat.shape[1])\n",
        "    # Build X, Y\n",
        "    X = []\n",
        "    Y = []\n",
        "    for (i, j) in dictionary:\n",
        "        X.append(src_mat[i])\n",
        "        Y.append(tgt_mat[j])\n",
        "    X = np.array(X)  # shape (num_pairs, 300)\n",
        "    Y = np.array(Y)  # shape (num_pairs, 300)\n",
        "\n",
        "    # Re-normalize just in case\n",
        "    X /= (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n",
        "    Y /= (np.linalg.norm(Y, axis=1, keepdims=True) + 1e-9)\n",
        "\n",
        "    # solve Y^T X => SVD\n",
        "    A = Y.T.dot(X)  # shape (300, 300)\n",
        "    U, s, Vt = np.linalg.svd(A)\n",
        "    W_refined = U.dot(Vt)\n",
        "    # if det < 0, fix sign\n",
        "    if np.linalg.det(W_refined) < 0:\n",
        "        U[:, -1] *= -1\n",
        "        W_refined = U.dot(Vt)\n",
        "    return W_refined\n",
        "\n",
        "\n",
        "# Building CSLS dictionary and refining\n",
        "\n",
        "# I am restricting to top_N most frequent words\n",
        "top_N = 50000\n",
        "src_top = min(top_N, mapped_en_embeddings.shape[0])\n",
        "tgt_top = min(top_N, hi_embeddings_normed.shape[0])\n",
        "\n",
        "src_sub = mapped_en_embeddings[:src_top]\n",
        "tgt_sub = hi_embeddings_normed[:tgt_top]\n",
        "# Building the dictionary with CSLS\n",
        "dictionary_mnn = build_dictionary_csls(src_sub, tgt_sub, K=50)\n",
        "print(f\"Found {len(dictionary_mnn)} mutual NN pairs via CSLS among top {top_N} words.\")\n",
        "\n",
        "# Solve orthogonal procrustes\n",
        "W_refined = orthogonal_procrustes_refine(dictionary_mnn, src_sub, tgt_sub)\n",
        "print(\"Procrustes refinement done.\")\n",
        "\n",
        "# Now W_refined is shape (300,300) that improves your initial W_np\n",
        "# Let's apply it to the full en_embeddings to get final mapping\n",
        "mapped_en_refined = en_embeddings.dot(W_refined)\n",
        "mapped_en_refined /= (\n",
        "    np.linalg.norm(mapped_en_refined, axis=1, keepdims=True) + 1e-9\n",
        ")\n",
        "\n",
        "\n",
        "# Final evaluation on test dictionary\n",
        "def evaluate_translation_csls(\n",
        "    test_pairs,\n",
        "    en_word2idx,\n",
        "    hi_word2idx,\n",
        "    mapped_en_matrix,\n",
        "    hi_matrix,\n",
        "    k=1\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate word translation using CSLS retrieval for each test pair.\n",
        "    For each English word e, we compute:\n",
        "       CSLS(e, h) = 2*cos(e, h) - rT(e) - rS(h)\n",
        "    and see if correct hi_word is in the top-k predictions.\n",
        "\n",
        "    We will do a simpler \"batch search\" approach:\n",
        "      1) Precompute rT, rS\n",
        "      2) For each test pair, re-compute CSLS scores, pick top-k, check correctness\n",
        "    \"\"\"\n",
        "    # precompute rT, rS over the entire mapped_en_matrix and hi_matrix\n",
        "    rT, rS = compute_rT_rS(mapped_en_matrix, hi_matrix, K=10)  # typically K=10\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for (en_w, hi_w) in test_pairs:\n",
        "        # check membership\n",
        "        if en_w not in en_word2idx or hi_w not in hi_word2idx:\n",
        "            continue\n",
        "        total += 1\n",
        "        en_idx = en_word2idx[en_w]\n",
        "        hi_idx = hi_word2idx[hi_w]\n",
        "\n",
        "        # compute csls score for each hi vector\n",
        "        x = mapped_en_matrix[en_idx]  # shape (300,)\n",
        "        cosines = x.dot(hi_matrix.T)  # shape (V_hi,)\n",
        "        # csls: 2*cos - rT(en_idx) - rS(h)\n",
        "        csls_scores = 2*cosines - rT[en_idx] - rS\n",
        "\n",
        "        # top-k\n",
        "        top_k_indices = np.argpartition(-csls_scores, k)[:k]\n",
        "        if hi_idx in top_k_indices:\n",
        "            correct += 1\n",
        "\n",
        "    return correct, total\n",
        "\n",
        "# Actually evaluate with P@1 and P@5\n",
        "c1, t1 = evaluate_translation_csls(test_pairs, en_word2idx, hi_word2idx,\n",
        "                                   mapped_en_refined, hi_embeddings_normed, k=1)\n",
        "c5, _ = evaluate_translation_csls(test_pairs, en_word2idx, hi_word2idx,\n",
        "                                  mapped_en_refined, hi_embeddings_normed, k=5)\n",
        "\n",
        "if t1 > 0:\n",
        "    print(f\"Final Unsupervised (Adv+CSLS) => P@1 = {c1/t1*100:.2f}%,  P@5 = {c5/t1*100:.2f}%\")\n",
        "else:\n",
        "    print(\"No valid test pairs found in vocab for evaluation!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2Lr1MX4oQKx",
        "outputId": "b42496b7-b479-46cd-bba9-57699f235e2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20755 mutual NN pairs via CSLS among top 50000 words.\n",
            "Procrustes refinement done.\n",
            "Final Unsupervised (Adv+CSLS) => P@1 = 0.00%,  P@5 = 0.01%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see here for **unsupervised CSLS and adversarial training**, we are getting P@1 = 0% and P@5 = 0.01%   \n",
        "\n",
        "In this assignment, we aligned English and Hindi word embeddings using a **supervised Procrustes approach**, achieving 26.07% P@1 and 51.87% P@5 translation accuracy on a standard test dictionary. By albation study we get,  \n",
        "Dictionary Size=5000: P@1=24.33% and P@5=47.87%  \n",
        "Dictionary Size=10000: P@1=25.53% and P@5=49.60%  \n",
        "Dictionary Size=15000: P@1=26.40% and P@5=49.80%  \n",
        "\n",
        "The reason behind unsupervised CSLS and adversarial training getting this much less precision is due to I mentioned increasing top_N to 50k for CSLS, but the adversarial training might still be under-trained.  \n",
        "\n",
        "The original MUSE approach commonly does tens of thousands of iterations with decaying learning rate and often multiple discriminator steps per generator step."
      ],
      "metadata": {
        "id": "WdA3Kn-w5oki"
      }
    }
  ]
}